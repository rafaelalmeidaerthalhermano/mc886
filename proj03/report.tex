\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{tikz}
\usepackage{pgfplots, pgfplotstable}
\usepackage{caption}
\usepackage{listings}
\lstset{language=Python}

\title{MC886 - Machine learning \\ Exercise 3}
\author{Rafael Almeida Erthal Hermano\\RA 121286}
\date{March 2014}

\begin{document}

\maketitle
\newpage

\section{The Data}
The data consists of 208 records with 60 float attributes divided in two categories M and R. To read the records the following function was implemented:

\begin{lstlisting}
def read_data(file_name):
    cast_int = lambda x: 1 if x == 'M' else 0
    lines    = open(file_name,'r').readlines()
    records  = [line[0:-1].split(',') for line in lines]
    data     = [map(float, record[0:-1]) for record in records]
    labels   = [cast_int(record[-1]) for record in records]
    return (data, labels)
\end{lstlisting}

\subsection{Normalization}
Since the range of values of raw data varies widely for all the attributes, all data were normalized. To normalize the data, the following algorithm was used:

\begin{lstlisting}
from sklearn.preprocessing import normalize

data = read_data("./data.csv")
data = (normalize(data[0]).tolist(), data[1])
\end{lstlisting}

\subsection{PCA}
To calculate the explained variance ratio for the PCA the following algorithm was used:

\begin{lstlisting}
from sklearn.decomposition import PCA
from sklearn.preprocessing import normalize

data = read_data("./data.csv")
data = (normalize(data[0]).tolist(), data[1])
pca  = PCA(n_components=12)
pca.fit(data[0])
print pca.explained_variance_ratio_
\end{lstlisting}

The explained variance ratio of PCA for the data is:

\begin{table}[h]
    \begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|}
        \hline
            0.325 & 0.230 & 0.082 & 0.054 & 0.050 & 0.045 & 0.028 & 0.024 & 0.022 & 0.018 & 0.016 & 0.014 \\ \hline
            0.010 & 0.010 & 0.008 & 0.008 & 0.007 & 0.006 & 0.005 & 0.005 & 0.004 & 0.003 & 0.003 & 0.003 \\ \hline
            0.002 & 0.002 & 0.002 & 0.002 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 \\ \hline
            0.001 & 0.001 & 0.001 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\ \hline
            0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\ \hline
    \end{tabular}
\end{table}

So to keep more than 90 percent of variance we should use 12 dimensions.

\newpage

\section{Cross validation}

To perform the grid search, the following function was implemented:

\begin{lstlisting}
def best_grid(data, grid, Classifier):
    best_k          = 0
    highest_matches = 0
    for K in grid:
        matches = 0
        for train_index, test_index in StratifiedKFold(data[1], 5):
            train = (
                [i for j, i in enumerate(data[0]) if j not in train_index],
                [i for j, i in enumerate(data[1]) if j not in train_index]
            )
            test  = (
                [i for j, i in enumerate(data[0]) if j not in test_index],
                [i for j, i in enumerate(data[1]) if j not in test_index]
            )
            classifier = Classifier(K)
            classifier.fit(train[0], train[1])
            matches += correct_predictions(classifier, test)
        if matches > highest_matches:
            best_k = K
            highest_matches = matches
    return best_k
\end{lstlisting}

And to calculate the matching rate for all folds the following function was implemented:

\begin{lstlisting}
def rates(data, grid, Classifier):
    rates    = []
    for train_index, test_index in StratifiedKFold(data[1], 5):
        train          = (
            [i for j, i in enumerate(data[0]) if j not in train_index],
            [i for j, i in enumerate(data[1]) if j not in train_index]
        )
        test           = (
            [i for j, i in enumerate(data[0]) if j not in test_index],
            [i for j, i in enumerate(data[1]) if j not in test_index]
        )
        hiperparameter = best_grid(train, grid, Classifier)
        classifier     = Classifier(hiperparameter)
        classifier.fit(train[0], train[1])
        rate = correct_predictions(classifier, test) / float(len(test[0]))
        rates.append(rate)
    return rates
\end{lstlisting}

\section{Algorithms}
The data was split into 5 folds, for each fold, the training data was split into another 5 folds, for each sub fold, was performed a grid search to calculate the best hiper parameters. After choosing the best hiper parameters, the entire training set of the original fold was used to choose the best hiper parameters for the final classifier, and after that calculated the match rate of the fold with the best hiper parameters.

\subsection{KNN}

\begin{table}[h]
    \begin{tabular}{|l|l|l|l|l|}
    \cline{1-2} \cline{4-5}
    \multicolumn{2}{|c|}{Without PCA} &  & \multicolumn{2}{|c|}{With PCA} \\ \cline{1-2} \cline{4-5}
    Fold match rate & Best K &  & Fold match rate & Best K  \\ \cline{1-2} \cline{4-5}
    0.67 & 3 &  & 0.66 & 3  \\ \cline{1-2} \cline{4-5}
    0.63 & 5 &  & 0.63 & 5  \\ \cline{1-2} \cline{4-5}
    0.78 & 1 &  & 0.74 & 1  \\ \cline{1-2} \cline{4-5}
    0.74 & 1 &  & 0.77 & 1  \\ \cline{1-2} \cline{4-5}
    0.78 & 1 &  & 0.79 & 1  \\ \cline{1-2} \cline{4-5}
    \end{tabular}
\end{table}


The mean for the KNN without PCA is $0.72 \pm 0.05$ and the mean for the KNN with PCA is $0.72 \pm 0.06$. So, for the KNN with this data it's irrelevant to use PCA and the hiper parameter doesn't vary a lot from each fold in both cases.

\subsection{Linear SVM}

\begin{table}[h]
    \begin{tabular}{|l|l|l|l|l|}
    \cline{1-2} \cline{4-5}
    \multicolumn{2}{|c|}{Without PCA} &  & \multicolumn{2}{|c|}{With PCA} \\ \cline{1-2} \cline{4-5}
    Fold match rate & Best C &  & Fold match rate & Best C  \\ \cline{1-2} \cline{4-5}
    0.72 & 100 &  & 0.75 & 100  \\ \cline{1-2} \cline{4-5}
    0.68 & 10  &  & 0.71 & 1000  \\ \cline{1-2} \cline{4-5}
    0.69 & 100 &  & 0.75 & 100  \\ \cline{1-2} \cline{4-5}
    0.72 & 100 &  & 0.76 & 1000  \\ \cline{1-2} \cline{4-5}
    0.70 & 100 &  & 0.70 & 100  \\ \cline{1-2} \cline{4-5}
    \end{tabular}
\end{table}

The mean for the linear SVM without PCA is $0.70 \pm 0.01$ and the mean for the linear SVM with PCA is $0.73 \pm 0.02$. So for the linear SVM, with this data, the PCA doesn't improve substantially the matching rate.
Its important to notice that after applying PCA, the hiper parameter C
began to vary more than without PCA.

\newpage

\subsection{RBF SVM}

\begin{table}[h]
    \begin{tabular}{|l|l|l|l|l|l|l|}
    \cline{1-3} \cline{5-7}
    \multicolumn{3}{|c|}{Without PCA} &  & \multicolumn{3}{|c|}{With PCA} \\ \cline{1-3} \cline{5-7}
    Fold match rate & Best C & Best Gamma &  & Fold match rate & Best C & Best Gamma  \\ \cline{1-3} \cline{5-7}
    0.74 & 1000  & 0.10 &  & 0.74 & 1000  & 0.10 \\ \cline{1-3} \cline{5-7}
    0.72 & 100   & 0.10 &  & 0.68 & 100   & 0.10 \\ \cline{1-3} \cline{5-7}
    0.68 & 10000 & 0.01 &  & 0.78 & 10000 & 0.01 \\ \cline{1-3} \cline{5-7}
    0.76 & 1000  & 0.10 &  & 0.76 & 1000  & 0.10 \\ \cline{1-3} \cline{5-7}
    0.77 & 100   & 1.00 &  & 0.76 & 1000  & 0.10 \\ \cline{1-3} \cline{5-7}
    \end{tabular}
\end{table}

The mean for the rbf SVM without PCA is $0.73 \pm 0.03$ and the mean for the rbf SVM with PCA is $0.74 \pm 0.03$. So for the rbf SVM with this data is irrelevant to use use PCA and the hiper parameters doesn't vary a lot from each fold.

\subsection{Random Forest}

\begin{table}[h]
    \begin{tabular}{|l|l|l|l|l|}
    \cline{1-2} \cline{4-5}
    \multicolumn{2}{|c|}{Without PCA} &  & \multicolumn{2}{|c|}{With PCA} \\ \cline{1-2} \cline{4-5}
    Fold match rate & Best mtry &  & Fold match rate & Best mtry  \\ \cline{1-2} \cline{4-5}
    0.69 & 20 &  & 0.66 & 10  \\ \cline{1-2} \cline{4-5}
    0.72 & 2  &  & 0.64 & 60  \\ \cline{1-2} \cline{4-5}
    0.70 & 10 &  & 0.71 & 5   \\ \cline{1-2} \cline{4-5}
    0.68 & 5  &  & 0.75 & 10  \\ \cline{1-2} \cline{4-5}
    0.70 & 5  &  & 0.73 & 3   \\ \cline{1-2} \cline{4-5}
    \end{tabular}
\end{table}

The mean for the random forest without PCA is $0.72 \pm 0.05$ and the mean for the random forest with PCA is $0.72 \pm 0.06$. So for the random forest with this data is irrelevant to use use PCA and the hiper parameter vary a lot from each fold.

\end{document}